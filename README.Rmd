---
title: "Readme"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=3)
require(algoeval)
```

`algoeval` does evaluation of algorithmic output that is independent of the underlying algorithms. The algorithmic output can come from any kind of model; these are then combined in a Bayesian hierarchical model where a restrictive prior is put on the predictor coefficients. The algorithms are then evaluated based on how well they perform compared to the full model and a null model that only predicts an intercept.

## How to use this package

This package can be downloaded and installed in R using the `devtools` package with the following command:

```
devtools::install_github('saudiwin/algoeval',local=FALSE)
```
All Stan code is compiled on install and then will be run within the package without recompiling.

A brief demonstration of the package follows.

First I simulate some algorithm data using default settings (3 algos).

```{r}
sample_algos <- algo_sim(N=2000)
```

This produces a list that can be then be put into the `compare_algos` function. `budget` and `revenue` are left at their default values. `budget` here refers to the amount that can be spent on advertising based on targeting from the different algorithms, while `revenue` is the average amount that a customer will spend given a purchase. The advertising budget will increase the average amount that a customer spends if they do decide to purchase. The marginal cost, `MC`, can also be adjusted. The default is set at 50% of revenue.

```{r}
metrics <- algo_compare(algos=sample_algos$algos,outcome=sample_algos$outcome)
```

The list produced by the `compare_algos` function includes the average difference between the full model and the null model for budgets and profits `$total_perf`, the difference of budgets and profits with the null model for each algorithm separately `$each_perf`, the LOO for the total model `$total_loo` and the LOO for each algorithm run separately `each_loo`.

The budget/profit figures can be examined by computing summary statistics on the vectors, which represent draws from the posterior of the model. We can look at mean values and also the SD of the mean to get a sense of uncertainty. For example, the mean profit difference between the full model and the null model is \$`r mean(metrics$total_perf$profit)` (SD \$`r sd(metrics$total_perf$profit)`) and the mean budget difference between the full model and the null model is \$`r mean(metrics$total_perf$budget)` (SD \$`r sd(metrics$total_perf$budget)`).

The LOO values can be reported as summary statistics. LOOIC is LOO as an information criterion (like BIC/AIC) and comes with a standard error around the estimate. For the full model LOOIC is `r unlist(metrics$total_loo[3])` with a standard error of `r unlist(metrics$total_loo[6])`.

To compare two models directly, use the `loo::compare()` function on two of the LOO values from the list:

```{r loo_compare}
compared <- loo::compare(metrics$total_loo,metrics$each_loo[[1]])
print(compared)
```

This shows that the difference is `r compared[1]` log density, which means that the total probability density of the first model (the total model) is higher than the first algorithm. This difference is statistically significant with a standard error of approximately `r compared[2]`.

A good way to visualize these metrics is by plotting them in comparison to each other. The plot function will create a chart with either profit, budget or LOO differences for the full versus individual models. The following is the plot of budget/profit differences between the full model and the individual models (numbers are relative to the null model).

The budget difference implies that using this algorithm, the optimal budget size for spending on advertising was less or more than the full model. Because the single algorithms perform less well than the full model, their optimal advertising budget sizes are also correspondingly lower, and they also have lower average profits.

```{r plot_profit}

algo_plot(obj=metrics,plot_type='cash')

```

Then we can also look at a plot comparing the full model to each individual model in terms of LOO.

```{r plot_loo}

algo_plot(obj=metrics,plot_type='loo')


```

Finally, we can calculate approximately how much we would pay each algorithm based on how much profit each algorithm produced and how much we have in our budget to spend on algorithms. The payout column shows payouts as a proportion of each algorithms contribution to total profit and then calculated as each algorithm's share of the budget for algorithms (set in the `algo_compare` function).

```{r algo_pay}

out_table <- algo_pay(obj=metrics)
knitr::kable(out_table)
```

